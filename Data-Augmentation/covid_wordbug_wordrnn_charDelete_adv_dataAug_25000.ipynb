{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNnf4gqV1GNr"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCUiXX-D1GNs"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SShSxj9-K9PZ"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGR0zILb1GNv"
   },
   "outputs": [],
   "source": [
    "if sys.version_info < (3,):\n",
    "    maketrans = string.maketrans\n",
    "else:\n",
    "    maketrans = str.maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    \n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    if sys.version_info < (3,):\n",
    "        if isinstance(text, unicode):\n",
    "            translate_map = dict((ord(c), unicode(split)) for c in filters)\n",
    "            text = text.translate(translate_map)\n",
    "        elif len(split) == 1:\n",
    "            translate_map = maketrans(filters, split * len(filters))\n",
    "            text = text.translate(translate_map)\n",
    "        else:\n",
    "            for c in filters:\n",
    "                text = text.replace(c, split)\n",
    "    else:\n",
    "        translate_dict = dict((c, split) for c in filters)\n",
    "        translate_map = maketrans(translate_dict)\n",
    "        text = text.translate(translate_map)\n",
    "\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_oYKHfI1GNy"
   },
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "\n",
    "    def __init__(self, num_words=None,\n",
    "                 filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                 lower=True,\n",
    "                 split=' ',\n",
    "                 char_level=False,\n",
    "                 oov_token=None,\n",
    "                 document_count=0,\n",
    "                 **kwargs):\n",
    "        # Legacy support\n",
    "        if 'nb_words' in kwargs:\n",
    "            warnings.warn('The `nb_words` argument in `Tokenizer` '\n",
    "                          'has been renamed `num_words`.')\n",
    "            num_words = kwargs.pop('nb_words')\n",
    "        if kwargs:\n",
    "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
    "\n",
    "        self.word_counts = OrderedDict()\n",
    "        self.word_docs = defaultdict(int)\n",
    "        self.filters = filters\n",
    "        self.split = split\n",
    "        self.lower = lower\n",
    "        self.num_words = num_words\n",
    "        self.document_count = document_count\n",
    "        self.char_level = char_level\n",
    "        self.oov_token = oov_token\n",
    "        self.index_docs = defaultdict(int)\n",
    "        self.word_index = dict()\n",
    "        self.index_word = dict()\n",
    "    def fit_on_texts(self, texts):\n",
    "        \n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                if self.lower:\n",
    "                    if isinstance(text, list):\n",
    "                        text = [text_elem.lower() for text_elem in text]\n",
    "                    else:\n",
    "                        text = text.lower()\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "                # In how many documents each word occurs\n",
    "                self.word_docs[w] += 1\n",
    "\n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        # forcing the oov_token to index 1 if it exists\n",
    "        if self.oov_token is None:\n",
    "            sorted_voc = []\n",
    "        else:\n",
    "            sorted_voc = [self.oov_token]\n",
    "        sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(\n",
    "            list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n",
    "\n",
    "        self.index_word = dict((c, w) for w, c in self.word_index.items())\n",
    "\n",
    "        for w, c in list(self.word_docs.items()):\n",
    "            self.index_docs[self.word_index[w]] = c\n",
    "\n",
    "    def fit_on_sequences(self, sequences):\n",
    "        self.document_count += len(sequences)\n",
    "        for seq in sequences:\n",
    "            seq = set(seq)\n",
    "            for i in seq:\n",
    "                self.index_docs[i] += 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return list(self.texts_to_sequences_generator(texts))\n",
    "\n",
    "    def texts_to_sequences_generator(self, texts):\n",
    "        num_words = self.num_words\n",
    "        oov_token_index = self.word_index.get(self.oov_token)\n",
    "        for text in texts:\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                if self.lower:\n",
    "                    if isinstance(text, list):\n",
    "                        text = [text_elem.lower() for text_elem in text]\n",
    "                    else:\n",
    "                        text = text.lower()\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                if i is not None:\n",
    "                    if num_words and i >= num_words:\n",
    "                        if oov_token_index is not None:\n",
    "                            vect.append(oov_token_index)\n",
    "                    else:\n",
    "                        vect.append(i)\n",
    "                elif self.oov_token is not None:\n",
    "                    vect.append(oov_token_index)\n",
    "            yield vect\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    \n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
    "\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" '\n",
    "                             'not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
    "                             'is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pbr75mAR1GN0"
   },
   "source": [
    "# loaddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNx75pMr1GN1"
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self,filename,line=3):\n",
    "        self.output = []\n",
    "        self.content = []\n",
    "        self.columns=line\n",
    "        self.loadcsv(filename)\n",
    "        \n",
    "    def loadcsv(self, filename):\n",
    "        reader = csv.reader(open(filename, \"rt\", encoding = \"utf8\"))\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            if self.columns==2:\n",
    "                self.output.append(int(row[0])-1)\n",
    "                self.content.append((row[1]).lower())\n",
    "            elif self.columns==3:\n",
    "                self.output.append(int(row[0])-1)\n",
    "                self.content.append((row[1] + \" \" + row[2]).lower())           \n",
    "            elif self.columns==4:\n",
    "                self.output.append(int(row[0])-1)\n",
    "                self.content.append((row[1] + \" \" + row[2] + \" \" + row[3]).lower())       \n",
    "def loaddata(i = 0):\n",
    "    datanames = ['covid'] \n",
    "    lines = [2]\n",
    "    classes= [2]\n",
    "    # if not 'blog' in datanames[i]:\n",
    "    trainadd = 'covid_wordrnn_charDelete_advTrain_25000.csv'\n",
    "    valadd='covid_val_7500.csv'\n",
    "    testadd = 'covid_test_17500.csv'\n",
    "    advtestadd='covid_wordrnn_charDelete_test_adv_samples.csv'\n",
    "\n",
    "    traindata = dataset(trainadd,lines[i])\n",
    "    valdata=dataset(valadd,lines[i])\n",
    "    testdata = dataset(testadd,lines[i])\n",
    "    advtestdata = dataset(advtestadd,lines[i])                  \n",
    "    return(traindata,valdata,testdata,advtestdata,classes[i])#valdata,\n",
    "    # else:\n",
    "    #     data = dataset()\n",
    "    #     return (traindata,testdata,classes[i])\n",
    "\n",
    "def loaddatawithtokenize(i = 0, nb_words = 20000, start_char = 1, oov_char=2, index_from=3, withraw = False, datalen = 500):\n",
    "    (traindata,valdata,testdata,advtestdata,numclass) = loaddata(i)\n",
    "    rawtrain = traindata.content[:]\n",
    "    rawval = valdata.content[:]\n",
    "    rawtest = testdata.content[:]\n",
    "    rawadvtest=advtestdata.content[:]\n",
    "\n",
    "    tokenizer = Tokenizer(lower=True)\n",
    "    tokenizer.fit_on_texts(traindata.content + valdata.content+ testdata.content+ advtestdata.content)\n",
    "\n",
    "    traindata.content = tokenizer.texts_to_sequences(traindata.content)\n",
    "    valdata.content  = tokenizer.texts_to_sequences(valdata.content)\n",
    "    testdata.content = tokenizer.texts_to_sequences(testdata.content)\n",
    "    advtestdata.content  = tokenizer.texts_to_sequences(advtestdata.content)\n",
    "    \n",
    "    if start_char==None:\n",
    "        traindata.content = [[w + index_from for w in x] for x in traindata.content]\n",
    "        valdata.content = [[w + index_from for w in x] for x in valdata.content]\n",
    "        testdata.content = [[w + index_from for w in x] for x in testdata.content]\n",
    "        advtestdata.content = [[w + index_from for w in x] for x in advtestdata.content]\n",
    "    else:\n",
    "        traindata.content = [[start_char]+[w + index_from for w in x] for x in traindata.content]\n",
    "        valdata.content = [[start_char]+[w + index_from for w in x] for x in valdata.content]\n",
    "        testdata.content = [[start_char]+[w + index_from for w in x] for x in testdata.content]\n",
    "        advtestdata.content = [[start_char]+[w + index_from for w in x] for x in advtestdata.content]\n",
    "    \n",
    "    traindata.content = [[w if w < nb_words else oov_char for w in x] for x in traindata.content]\n",
    "    valdata.content = [[w if w < nb_words else oov_char for w in x] for x in valdata.content]\n",
    "    testdata.content = [[w if w < nb_words else oov_char for w in x] for x in testdata.content]\n",
    "    advtestdata.content = [[w if w < nb_words else oov_char for w in x] for x in advtestdata.content]\n",
    "    \n",
    "    traindata.content = pad_sequences(traindata.content, maxlen=datalen)\n",
    "    valdata.content = pad_sequences(valdata.content, maxlen=datalen)\n",
    "    testdata.content = pad_sequences(testdata.content, maxlen=datalen)\n",
    "    advtestdata.content = pad_sequences(advtestdata.content, maxlen=datalen)\n",
    "    if withraw:\n",
    "        return traindata,valdata,testdata,advtestdata,tokenizer,numclass,rawtrain,rawtest\n",
    "    else:\n",
    "        return traindata,valdata,testdata,advtestdata,tokenizer,numclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5VPGHfx1GN3"
   },
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClJEqkQ71GN6"
   },
   "outputs": [],
   "source": [
    "class Worddata(Dataset):\n",
    "    def __init__(self, data, tokenizer = True, length=1014, space = False, backward = -1, getidx = False, rawdata = None):\n",
    "        self.backward = backward\n",
    "        self.length = length\n",
    "        (self.inputs,self.labels) = (data.content,data.output)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        self.inputs = torch.from_numpy(self.inputs).long()\n",
    "        self.getidx = getidx\n",
    "        if rawdata:\n",
    "            self.raw = rawdata\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.inputs[idx]\n",
    "        y = self.labels[idx]\n",
    "        if self.getidx==True:\n",
    "            if self.raw:\n",
    "                return x,y,idx,self.raw[idx]\n",
    "            else:\n",
    "                return x,y,idx\n",
    "        else:\n",
    "            return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PFPKNDi1GN8"
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Tc1xR6G81GN9",
    "outputId": "30f8d8f1-88cf-446a-9c87-1d9cec95a485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqlZf0Jq1GOD"
   },
   "outputs": [],
   "source": [
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, classes=2, bidirection = False, layernum=1, length=20000,embedding_size =100, hiddensize = 100):\n",
    "        super(WordRNN, self).__init__()\n",
    "        self.embd = nn.Embedding(length, embedding_size)\n",
    "        # self.lstm = nn.LSTMCell(hiddensize, hiddensize)\n",
    "        self.lstm = nn.LSTM(embedding_size, hiddensize, layernum, bidirectional = bidirection)\n",
    "        self.hiddensize = hiddensize\n",
    "        numdirections = 1 + bidirection\n",
    "        self.hsize = numdirections * layernum\n",
    "        self.linear = nn.Linear(hiddensize * numdirections, classes)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "    def forward(self, x, returnembd = False):\n",
    "        embd = self.embd(x)\n",
    "        if returnembd:\n",
    "            embd = Variable(embd.data, requires_grad=True).to(device)\n",
    "            embd.retain_grad()\n",
    "            # print embd.size()\n",
    "        h0 = Variable(torch.zeros(self.hsize, embd.size(0), self.hiddensize)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.hsize, embd.size(0), self.hiddensize)).to(device)\n",
    "        # for inputs in x:\n",
    "        x = embd.transpose(0,1)\n",
    "        x,(hn,cn) = self.lstm(x,(h0,c0))\n",
    "        x = x[-1]\n",
    "        # x = x[-1].transpose(0,1)\n",
    "        # x = x.view(x.size(0),-1)\n",
    "        x = self.log_softmax(self.linear(x))\n",
    "        if returnembd:\n",
    "            return embd,x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXCkr8TI1GOG"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.dat'):\n",
    "    torch.save(state, filename + '_checkpoint.dat')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_checkpoint.dat', filename + \"_bestmodel.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ov14woue1GOJ"
   },
   "outputs": [],
   "source": [
    "data=0\n",
    "wordlength=200\n",
    "dictionarysize=20000\n",
    "batchsize=128\n",
    "backward=-1\n",
    "epochs=5\n",
    "power=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IODvOagM1GOL",
    "outputId": "0780433f-6332-458f-e7f5-bba6e76766a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed_all(7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Loading data..\")\n",
    "(train,val,test,advtest,tokenizer,numclass)=loaddatawithtokenize(data,nb_words=dictionarysize,datalen=wordlength)\n",
    "word_index = tokenizer.word_index\n",
    "trainword = Worddata(train,backward = backward)\n",
    "valword = Worddata(val,backward = backward)\n",
    "testword = Worddata(test,backward = backward)\n",
    "advtestword = Worddata(advtest,backward = backward)\n",
    "train_loader = DataLoader(trainword,batch_size=batchsize, shuffle = True)#, num_workers=4\n",
    "val_loader = DataLoader(valword,batch_size=batchsize, shuffle = True)#, num_workers=4\n",
    "test_loader = DataLoader(testword,batch_size=batchsize)#, num_workers=4\n",
    "advtest_loader = DataLoader(advtestword,batch_size=batchsize)#, num_workers=4\n",
    "\n",
    "maxlength =wordlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRgmPLqV1GOR"
   },
   "outputs": [],
   "source": [
    "model = WordRNN(classes = 2)\n",
    "model = model.to(device)\n",
    "#print(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "CTQTynkm1GOU",
    "outputId": "a471a322-0105-4ff5-ddf7-4b5d24085961"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train_Loss: 0.0044 Train_Accuracy: 0.69725 \n",
      "Validation_Loss: 0.0040 Validation_Accuracy: 0.73308\n",
      "Epoch 2 :\n",
      "Train_Loss: 0.0030 Train_Accuracy: 0.82918 \n",
      "Validation_Loss: 0.0032 Validation_Accuracy: 0.81381\n",
      "Epoch 3 :\n",
      "Train_Loss: 0.0021 Train_Accuracy: 0.89420 \n",
      "Validation_Loss: 0.0029 Validation_Accuracy: 0.84626\n",
      "Epoch 4 :\n",
      "Train_Loss: 0.0014 Train_Accuracy: 0.93177 \n",
      "Validation_Loss: 0.0030 Validation_Accuracy: 0.85184\n",
      "Epoch 5 :\n",
      "Train_Loss: 0.0010 Train_Accuracy: 0.95585 \n",
      "Validation_Loss: 0.0031 Validation_Accuracy: 0.86607\n"
     ]
    }
   ],
   "source": [
    "bestacc = 0\n",
    "beta=1\n",
    "for epoch in range(epochs):\n",
    "    #print('Start epoch %d' % epoch)\n",
    "    model.train()\n",
    "    correct_train = .0\n",
    "    correct_adv = .0\n",
    "    total_loss_train = 0\n",
    "    for dataid, data in enumerate(train_loader):\n",
    "        inputs,target = data\n",
    "        inputs,target = Variable(inputs),  Variable(target)\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        natural_loss=F.nll_loss(output, target)\n",
    "        pred_train = torch.max(output, 1)[1].view(target.size())\n",
    "        #pred_train = output.data.max(1, keepdim=True)[1]\n",
    "        correct_train += (pred_train == target).sum().item()\n",
    "        \n",
    "        #advinputs=generate_adv(model, inputs, pred_train, numclass)\n",
    "        #output_adv = model(advinputs)\n",
    "        #adv_loss=F.nll_loss(output_adv, target)\n",
    "        #pred_adv = torch.max(output_adv, 1)[1].view(target.size())\n",
    "        #correct_adv += (pred_adv == target).sum().item()\n",
    "        \n",
    "        loss =natural_loss #+ beta *(adv_loss)\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    correct_val= .0\n",
    "    total_loss_val = 0\n",
    "    model.eval()\n",
    "    for dataid, data in enumerate(val_loader):\n",
    "        inputs,target = data\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        total_loss_val += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct_val += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    acc_train = correct_train/len(train_loader.dataset)\n",
    "    #acc_train_adv=correct_adv/len(train_loader.dataset)\n",
    "    avg_loss_train = total_loss_train/len(train_loader.dataset)\n",
    "    acc_val = correct_val/len(val_loader.dataset)\n",
    "    avg_loss_val = total_loss_val/len(val_loader.dataset)\n",
    "    print('Epoch %d :'%(epoch+1))\n",
    "    print('Train_Loss: %.4f Train_Accuracy: %.5f ' % (avg_loss_train,acc_train))\n",
    "    print('Validation_Loss: %.4f Validation_Accuracy: %.5f' % (avg_loss_val,acc_val))\n",
    "    is_best = acc_val > bestacc\n",
    "    if is_best:\n",
    "        bestacc = acc_val\n",
    "    if dictionarysize!=20000:\n",
    "        fname = \"covid_wordrnn_adv\" +str(dictionarysize) + \"_\" \n",
    "    else:\n",
    "        fname = \"covid_wordrnn_adv\" + \"_\" \n",
    "        \n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'bestacc': bestacc,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filename = fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmoefP8ebE2E"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPqZp0hTZToy"
   },
   "outputs": [],
   "source": [
    "modelpath='covid_wordrnn_adv__bestmodel.dat'\n",
    "model = WordRNN(classes = 2)\n",
    "\n",
    "state = torch.load(modelpath)\n",
    "model = model.to(device)\n",
    "try:\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "except:\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "IsikSv-ZbzhL",
    "outputId": "d437b551-0e0e-4ba5-9038-4c63d781f844"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 17500 clean test images: 86.40 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 17500 clean test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "0DG_ICUHbVd4",
    "outputId": "05e7504e-c5e3-4fb0-8617-cf10a57ad0d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 17500 adversarial test images: 81.85 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "lab=[]\n",
    "pred=[]\n",
    "with torch.no_grad():\n",
    "    for data in advtest_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        #print(labels.tolist(),predicted.tolist())\n",
    "        lab.extend(labels)\n",
    "        pred.extend(predicted)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 17500 adversarial test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5H_-gPnB05fN"
   },
   "outputs": [],
   "source": [
    "lab=[i.item() for i in lab]\n",
    "pred=[i.item() for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "Yl1m4ej2f5Xm",
    "outputId": "3c455891-f049-4556-e2c8-802d59d09a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77      8748\n",
      "           1       0.79      0.69      0.74      8796\n",
      "\n",
      "    accuracy                           0.76     17544\n",
      "   macro avg       0.76      0.76      0.75     17544\n",
      "weighted avg       0.76      0.76      0.75     17544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#target_names = ['class 0', 'class 1']\n",
    "print(classification_report(lab, pred))#, target_names=target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2tf3R_8z2zi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "covid_wordbug_wordrnn_charDelete_adv_dataAug_25000.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
